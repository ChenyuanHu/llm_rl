import os
import time
import torch
import numpy as np
from typing import Dict, List
from transformers import set_seed
from torch.optim import AdamW
import torch.nn.functional as F
from torch.utils.data import DataLoader

from config import TrainingConfig
from utils import (
    load_math_dataset, extract_answer, extract_predicted_answer,
    compute_reward, save_metrics, setup_model_and_tokenizer,
    MetricsTracker, format_math_chat_input
)

class TrainingUtils:
    """è®­ç»ƒå·¥å…·ç±» - åˆ†ç¦»å·¥å…·æ–¹æ³•"""
    
    @staticmethod
    def create_dataloader(dataset, config):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        batch_size = config.per_device_train_batch_size * config.gradient_accumulation_steps
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            collate_fn=lambda batch: batch
        ), batch_size

    @staticmethod
    def log_training_step(epoch, total_epochs, batch_idx, total_batches, 
                         stats, cost_time):
        """ç»Ÿä¸€çš„è®­ç»ƒæ—¥å¿—è¾“å‡º"""
        print(f"Epoch {epoch}/{total_epochs}, "
              f"Batch {batch_idx}/{total_batches}, "
              f"Tokens: {stats['avg_tokens']:.1f}, "
              f"Reward: {stats['avg_reward']:.3f}, "
              f"Loss: {stats['avg_loss']:.4f}, "
              f"LogProb: {stats.get('avg_log_prob', 0):.3f}, "
              f"Time: {cost_time:.2f}s")

    @staticmethod
    def save_checkpoint(model, tokenizer, epoch, output_dir):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = os.path.join(output_dir, f"checkpoint-epoch-{epoch}")
        os.makedirs(checkpoint_dir, exist_ok=True)
        model.save_pretrained(checkpoint_dir)
        tokenizer.save_pretrained(checkpoint_dir)
        return checkpoint_dir

class SimpleGRPOTrainer:
    """ç®€åŒ–çš„GRPOè®­ç»ƒå™¨ - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.device = config.get_device()
        self.metrics_tracker = MetricsTracker()
        self.model = None
        self.tokenizer = None
        self.optimizer = None
        
        # åˆå§‹åŒ–
        set_seed(config.seed)
        self._print_device_info()
        
    def _print_device_info(self):
        """æ‰“å°è®¾å¤‡ä¿¡æ¯"""
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        if self.config.use_mps:
            print("MPSå¯ç”¨ï¼Œå°†ä½¿ç”¨Apple Silicon GPUåŠ é€Ÿ")
        
    def setup(self):
        """ç»Ÿä¸€çš„è®¾ç½®æ–¹æ³•"""
        print("æ­£åœ¨è®¾ç½®æ¨¡å‹å’Œæ•°æ®...")
        
        # è®¾ç½®æ¨¡å‹
        self.model, self.tokenizer = setup_model_and_tokenizer(self.config)
        self.model.to(self.device)

        self.ref_model, _ = setup_model_and_tokenizer(self.config, load_tokenizer=False)
        self.ref_model.to(self.device)
        
        # å†»ç»“å‚è€ƒæ¨¡å‹
        for param in self.ref_model.parameters():
            param.requires_grad = False
        self.ref_model.eval()
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        self.optimizer = AdamW(
            self.model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )
        
        # è®¾ç½®æ•°æ®é›†
        self.dataset = load_math_dataset(self.config)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.config.output_dir, exist_ok=True)
        os.makedirs(self.config.logging_dir, exist_ok=True)
        
        print(f"è®¾ç½®å®Œæˆ - æ¨¡å‹å·²åŠ è½½ï¼Œæ•°æ®é›†: {len(self.dataset)}æ¡æ ·æœ¬")
        
    def generate_and_evaluate_batch(self, batch_data: List[Dict]) -> List[Dict]:
        """æ‰¹é‡ç”Ÿæˆå“åº”å¹¶è¯„ä¼° - ä¼˜åŒ–ç‰ˆæœ¬"""
        # å‡†å¤‡æ‰¹é‡è¾“å…¥
        prompts = []
        questions = []
        ground_truths = []
        
        for item in batch_data:
            question = item['question']
            ground_truth = extract_answer(item['answer'])
            prompt = format_math_chat_input(question, self.tokenizer)
            
            prompts.append(prompt)
            questions.append(question)
            ground_truths.append(ground_truth)
        
        # æ‰¹é‡tokenize
        inputs = self.tokenizer(
            prompts, 
            return_tensors="pt", 
            padding=True,
            truncation=True, 
            max_length=self.config.max_length
        ).to(self.device)
        
        # æ‰¹é‡ç”Ÿæˆå“åº”
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config.max_new_tokens,
                do_sample=True,
                temperature=0.5,
                # top_k=10,
                # top_p=0.9,
                # repetition_penalty=1.1,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        # å¤„ç†æ‰¹é‡ç»“æœ
        results = []
        input_length = inputs['input_ids'].shape[1]
        
        for i in range(len(batch_data)):
            # æå–å“åº”éƒ¨åˆ†
            response_ids = outputs[i][input_length:]
            
            # å®‰å…¨çš„EOS tokenæŸ¥æ‰¾ - ä½¿ç”¨torchæ“ä½œè€ŒéPython list
            eos_mask = (response_ids == self.tokenizer.eos_token_id)
            if eos_mask.any():
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªEOS tokençš„ä½ç½®
                eos_positions = torch.nonzero(eos_mask, as_tuple=False)
                if len(eos_positions) > 0:
                    last_eos_index = eos_positions[0].item()
                    response_ids = response_ids[:last_eos_index]
            # å¦‚æœæ²¡æœ‰EOS tokenï¼Œä¿æŒåŸå§‹é•¿åº¦ï¼ˆè¿™æ˜¯æ­£å¸¸æƒ…å†µï¼‰
            
            # è§£ç å“åº”
            response = self.tokenizer.decode(response_ids, skip_special_tokens=False)
            
            # è¯„ä¼°å“åº”
            predicted_answer = extract_predicted_answer(response)
            # ä¼˜åŒ–å¥–åŠ±è®¡ç®— - ä½¿ç”¨tensoré•¿åº¦è€ŒéPython len()
            # token_penalty = 0.005 * response_ids.shape[0]  # è¿‡é•¿æ€è€ƒæƒ©ç½š
            token_penalty = 0
            reward = compute_reward(predicted_answer, ground_truths[i]) - token_penalty
            
            results.append({
                'prompt': prompts[i],
                'response': response,
                'token_count': response_ids.shape[0],  # ä½¿ç”¨tensor.shape[0]è€Œélen()
                'input_ids': inputs['input_ids'][i:i+1],  # ä¿æŒbatchç»´åº¦
                'response_ids': response_ids,
                'predicted_answer': predicted_answer,
                'ground_truth': ground_truths[i],
                'reward': reward
            })
            
        return results
    
    def compute_grpo_loss(self, batch_results: List[Dict]):
        """å®Œæ•´çš„GRPOæŸå¤±è®¡ç®— - å®Œå…¨å¯¹é½åŸå§‹è®ºæ–‡å®ç°
        
        GRPO (Group Relative Policy Optimization) åŸå§‹è®ºæ–‡å…¬å¼ï¼š
        L = E[min(r(Î¸) * A, clip(r(Î¸), 1-Îµ, 1+Îµ) * A)] - Î² * KL(Ï€_Î¸ || Ï€_ref)
        
        å…¶ä¸­ï¼š
        - r(Î¸) = Ï€_Î¸(y|x) / Ï€_ref(y|x) æ˜¯é‡è¦æ€§é‡‡æ ·æ¯”ç‡
        - A æ˜¯ç›¸å¯¹ä¼˜åŠ¿ (groupå†…z-scoreæ ‡å‡†åŒ–)
        - clip() æ˜¯PPOé£æ ¼çš„clipping
        - KLæ˜¯å¯¹å‚è€ƒç­–ç•¥çš„KLæ•£åº¦çº¦æŸ
        """
        if len(batch_results) == 0:
            return None, 0.0
            
        # å‡†å¤‡æ•°æ®
        all_input_ids = []
        all_response_ids = []
        all_rewards = []
        
        for result in batch_results:
            if len(result['response_ids']) > 0:
                all_input_ids.append(result['input_ids'].squeeze(0))
                all_response_ids.append(result['response_ids'])
                all_rewards.append(result['reward'])
        
        if len(all_rewards) == 0:
            return None, 0.0
            
        # è½¬æ¢ä¸ºtensor
        rewards = torch.tensor(all_rewards, dtype=torch.float32, device=self.device)

        # GRPOæ ¸å¿ƒï¼šè®¡ç®—groupå†…çš„relative advantage (z-scoreæ ‡å‡†åŒ–)
        if len(rewards) > 1:
            reward_mean = rewards.mean()
            reward_std = rewards.std() + 1e-8  # é¿å…é™¤é›¶
            advantages = (rewards - reward_mean) / reward_std
        else:
            advantages = rewards
        
        # é«˜æ•ˆæ‰¹å¤„ç†ï¼šå‡†å¤‡æ‰¹é‡æ•°æ®
        batch_full_ids = []
        batch_prompt_lengths = []
        batch_response_lengths = []
        
        for input_ids, response_ids in zip(all_input_ids, all_response_ids):
            if response_ids.dim() == 1:
                response_ids = response_ids.unsqueeze(0)
            if input_ids.dim() == 1:
                input_ids = input_ids.unsqueeze(0)
                
            full_ids = torch.cat([input_ids, response_ids], dim=1)
            batch_full_ids.append(full_ids.squeeze(0))
            batch_prompt_lengths.append(input_ids.shape[1])
            batch_response_lengths.append(response_ids.shape[1])
        
        # åºåˆ—paddingå’Œæ‰¹å¤„ç†
        max_length = max(seq.shape[0] for seq in batch_full_ids)
        padded_batch = torch.full(
            (len(batch_full_ids), max_length), 
            self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id,
            dtype=torch.long,
            device=self.device
        )
        
        attention_mask = torch.zeros_like(padded_batch)
        
        for i, seq in enumerate(batch_full_ids):
            seq_len = seq.shape[0]
            padded_batch[i, :seq_len] = seq
            attention_mask[i, :seq_len] = 1
        
        # å½“å‰ç­–ç•¥å‰å‘ä¼ æ’­ï¼ˆä¿ç•™æ¢¯åº¦ï¼‰
        current_outputs = self.model(padded_batch, attention_mask=attention_mask)
        
        # å‚è€ƒç­–ç•¥å‰å‘ä¼ æ’­ï¼ˆæ— æ¢¯åº¦ï¼‰
        with torch.no_grad():
            ref_outputs = self.ref_model(padded_batch, attention_mask=attention_mask)
        
        # GRPOæŸå¤±è®¡ç®—
        total_policy_loss = 0
        total_kl_loss = 0
        total_log_prob = 0
        valid_samples = 0
        
        # GRPOè¶…å‚æ•°
        clip_epsilon = self.config.clip_epsilon
        kl_coeff = self.config.kl_coeff
        
        for i, (prompt_length, response_length, advantage) in enumerate(
            zip(batch_prompt_lengths, batch_response_lengths, advantages)
        ):
            # æå–responseéƒ¨åˆ†çš„logits
            current_logits = current_outputs.logits[i]
            ref_logits = ref_outputs.logits[i]
            
            response_start = prompt_length - 1  # autoregressive shift
            response_end = prompt_length + response_length - 1
            
            current_response_logits = current_logits[response_start:response_end]
            ref_response_logits = ref_logits[response_start:response_end]
            
            # æå–response token ids
            response_targets = padded_batch[i][prompt_length:prompt_length + response_length]
            
            # è®¡ç®—å½“å‰ç­–ç•¥çš„log probabilities
            current_log_probs = F.log_softmax(current_response_logits, dim=-1)
            current_selected_log_probs = current_log_probs.gather(1, response_targets.unsqueeze(1)).squeeze()
            current_log_prob_sum = current_selected_log_probs.sum()
            
            # è®¡ç®—å‚è€ƒç­–ç•¥çš„log probabilities
            ref_log_probs = F.log_softmax(ref_response_logits, dim=-1)
            ref_selected_log_probs = ref_log_probs.gather(1, response_targets.unsqueeze(1)).squeeze()
            ref_log_prob_sum = ref_selected_log_probs.sum()
            
            # è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”ç‡ r(Î¸) = Ï€_Î¸(y|x) / Ï€_ref(y|x)
            log_ratio = current_log_prob_sum - ref_log_prob_sum
            ratio = torch.exp(log_ratio)
            
            # PPO/GRPO clipping
            clipped_ratio = torch.clamp(ratio, 1.0 - clip_epsilon, 1.0 + clip_epsilon)
            
            # è®¡ç®—clipped surrogate objective
            # L_clip = E[min(r(Î¸) * A, clip(r(Î¸), 1-Îµ, 1+Îµ) * A)]
            surrogate1 = ratio * advantage
            surrogate2 = clipped_ratio * advantage
            policy_loss = -torch.min(surrogate1, surrogate2)  # è´Ÿå·å› ä¸ºæˆ‘ä»¬è¦æœ€å¤§åŒ–
            
            # è®¡ç®—KLæ•£åº¦
            # KL(Ï€_Î¸ || Ï€_ref) åœ¨response tokensä¸Š  
            # PyTorch KLæ•£åº¦: F.kl_div(log_probs, target_probs)
            ref_probs = F.softmax(ref_response_logits, dim=-1)
            kl_divergence = F.kl_div(current_log_probs, ref_probs, reduction='sum')
            
            total_policy_loss += policy_loss
            total_kl_loss += kl_divergence
            total_log_prob += current_log_prob_sum.item()
            valid_samples += 1
        
        if valid_samples == 0:
            return None, 0.0
        
        # è®¡ç®—æœ€ç»ˆçš„GRPOæŸå¤±
        # L_total = L_clip - Î² * KL(Ï€_Î¸ || Ï€_ref)
        avg_policy_loss = total_policy_loss / valid_samples
        avg_kl_loss = total_kl_loss / valid_samples
        
        # åŸå§‹GRPOè®ºæ–‡çš„å®Œæ•´æŸå¤±å…¬å¼
        total_loss = avg_policy_loss + kl_coeff * avg_kl_loss
        
        avg_log_prob = total_log_prob / valid_samples
        
        # è¯¦ç»†è°ƒè¯•ä¿¡æ¯
        if len(rewards) > 1:
            print(f"  GRPO Complete Stats:")
            print(f"    Rewards: {rewards}, [{rewards.min():.3f}, {rewards.max():.3f}], Mean: {rewards.mean():.3f}, Std: {rewards.std():.3f}")
            print(f"    Advantages: {advantages}, [{advantages.min():.3f}, {advantages.max():.3f}]")
            print(f"    Policy Loss: {avg_policy_loss:.4f}, KL Loss: {avg_kl_loss:.4f}")
            print(f"    Total Loss: {total_loss:.4f}, Log Prob: {avg_log_prob:.3f}")
        
        return total_loss, avg_log_prob
    
    def train_group(self, group_data) -> Dict:
        """æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤ - æ‰¹å¤„ç†GPUä¼˜åŒ–ç‰ˆæœ¬"""
        # æ‰¹é‡ç”Ÿæˆå’Œè¯„ä¼°
        batch_results = self.generate_and_evaluate_batch(group_data)
        print(f"batch_results: {batch_results}")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_reward = sum(result['reward'] for result in batch_results)
        total_tokens = sum(result['token_count'] for result in batch_results)
        batch_size = len(batch_results)
        
        # æ‰“å°è¯¦ç»†ä¿¡æ¯ï¼ˆæŠ½æ ·æ˜¾ç¤ºï¼‰
        negative_count = sum(1 for r in batch_results if r['reward'] < 0)
        sample_displayed = 0
        max_display = 0  # æœ€å¤šæ˜¾ç¤º3ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
        
        for i, result in enumerate(batch_results):
            should_display = sample_displayed < max_display
            if should_display:
                print(f"Sample {i+1}/{batch_size}:")
                print(f"  predicted_answer: {result['predicted_answer']}")
                print(f"  ground_truth: {result['ground_truth']}")
                print(f"  reward: {result['reward']}")
                print(f"  token_count: {result['token_count']}")
                if result['reward'] < 0:
                    print(f"  prompt: {result['prompt'][:100]}...")
                    print(f"  response: {result['response'][:100]}...")
                sample_displayed += 1
        
        print(f"è´Ÿå¥–åŠ±æ ·æœ¬æ•°: {negative_count}/{batch_size}")
        
        # ä½¿ç”¨GRPOè®¡ç®—æŸå¤±
        loss, avg_log_prob = self.compute_grpo_loss(batch_results)
        
        if loss is not None:
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)
            self.optimizer.step()
            
            loss_value = loss.item()
        else:
            loss_value = 0.0
        
        # è¿”å›å¹³å‡ç»Ÿè®¡
        return {
            'avg_loss': loss_value,
            'avg_reward': total_reward / batch_size,
            'avg_tokens': total_tokens / batch_size,
            'avg_log_prob': avg_log_prob if loss is not None else 0.0
        }

    def sample_group(self, batch_data) -> List[Dict]:
        """GRPO groupé‡‡æ · - ä¸ºå•ä¸ªpromptåˆ›å»ºgroup_sizeä¸ªå‰¯æœ¬ä¾›ç”Ÿæˆå¤šä¸ªresponses"""
        assert len(batch_data) == 1
        
        # ä¸ºåŒä¸€ä¸ªpromptåˆ›å»ºgroup_sizeä¸ªå‰¯æœ¬
        # generate_and_evaluate_batchä¼šä¸ºæ¯ä¸ªå‰¯æœ¬ç”Ÿæˆä¸åŒçš„response
        return [batch_data[0]] * self.config.group_size
        
    def train_epoch(self, epoch: int) -> Dict:
        """è®­ç»ƒä¸€ä¸ªepoch - ç®€åŒ–ç‰ˆæœ¬"""
        print(f"\nå¼€å§‹è®­ç»ƒ Epoch {epoch + 1}/{self.config.num_train_epochs}")
        
        self.model.train()
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader, batch_size = TrainingUtils.create_dataloader(self.dataset, self.config)
        print(f"æ‰¹å¤„ç†å¤§å°: {batch_size}, æ‰¹æ¬¡æ•°: {len(dataloader)}")
        
        # è®­ç»ƒæŒ‡æ ‡
        epoch_metrics = {'token_counts': [], 'rewards': [], 'losses': []}
        
        for batch_idx, batch in enumerate(dataloader):
            # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
            start_time = time.time()
            group = self.sample_group(batch)
            step_stats = self.train_group(group)
            cost_time = time.time() - start_time
            
            # è®°å½•æŒ‡æ ‡
            current_step = epoch * len(dataloader) + batch_idx
            self.metrics_tracker.add_metrics(
                current_step, step_stats['avg_tokens'], 
                step_stats['avg_reward'], step_stats['avg_loss']
            )
            
            epoch_metrics['token_counts'].append(step_stats['avg_tokens'])
            epoch_metrics['rewards'].append(step_stats['avg_reward'])
            epoch_metrics['losses'].append(step_stats['avg_loss'])
            if 'avg_log_prob' in step_stats:
                if 'log_probs' not in epoch_metrics:
                    epoch_metrics['log_probs'] = []
                epoch_metrics['log_probs'].append(step_stats['avg_log_prob'])
            
            # æ—¥å¿—è¾“å‡º
            if (batch_idx + 1) % self.config.logging_steps == 0:
                TrainingUtils.log_training_step(
                    epoch + 1, self.config.num_train_epochs,
                    batch_idx + 1, len(dataloader),
                    step_stats, cost_time
                )
        
        return {f"avg_{k[:-1]}": np.mean(v) for k, v in epoch_metrics.items()}
        
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯ - ç®€åŒ–ç‰ˆæœ¬"""
        print("="*60)
        print("å¼€å§‹GRPOè®­ç»ƒ")
        print("="*60)
        
        # ç»Ÿä¸€è®¾ç½®
        self.setup()
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.config.num_train_epochs):
            epoch_stats = self.train_epoch(epoch)
            
            # è¾“å‡ºepochç»Ÿè®¡
            print(f"\nEpoch {epoch + 1} å®Œæˆ:")
            for key, value in epoch_stats.items():
                print(f"  {key}: {value:.3f}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            checkpoint_dir = TrainingUtils.save_checkpoint(
                self.model, self.tokenizer, epoch + 1, self.config.output_dir
            )
            print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_dir}")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self._save_final_results()
        print("è®­ç»ƒå®Œæˆï¼")
        
    def _save_final_results(self):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_dir = os.path.join(self.config.output_dir, "final_model")
        self.model.save_pretrained(final_model_dir)
        self.tokenizer.save_pretrained(final_model_dir)
        
        # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
        metrics_dict = self.metrics_tracker.save_to_dict()
        save_metrics(metrics_dict, self.config.output_dir)
        
        print(f"æœ€ç»ˆæ¨¡å‹ä¿å­˜è‡³: {final_model_dir}")

def main():
    """ä¸»å‡½æ•° - ç®€åŒ–ç‰ˆæœ¬"""
    print("ğŸš€ åˆå§‹åŒ–GRPOè®­ç»ƒ...")
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨å¹¶è®­ç»ƒ
        config = TrainingConfig()
        trainer = SimpleGRPOTrainer(config)
        trainer.train()
        
        # è®­ç»ƒæ€»ç»“
        print("\nğŸ“Š è®­ç»ƒæ€»ç»“:")
        final_stats = trainer.metrics_tracker.get_averages()
        for key, value in final_stats.items():
            print(f"  {key}: {value:.3f}")
        
        print("\nğŸ’¡ è¿è¡Œå¯è§†åŒ–: python visualize.py")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        raise

if __name__ == "__main__":
    main() 