import os
import time
import torch
import numpy as np
from typing import Dict, List
from transformers import set_seed
from torch.optim import AdamW
import torch.nn.functional as F
from torch.utils.data import DataLoader

from config import TrainingConfig
from utils import (
    load_math_dataset, extract_answer, extract_predicted_answer,
    compute_reward, save_metrics, setup_model_and_tokenizer,
    MetricsTracker, format_math_chat_input
)

class TrainingUtils:
    """è®­ç»ƒå·¥å…·ç±» - åˆ†ç¦»å·¥å…·æ–¹æ³•"""
    
    @staticmethod
    def create_dataloader(dataset, config):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        batch_size = config.per_device_train_batch_size * config.gradient_accumulation_steps
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            collate_fn=lambda batch: batch
        ), batch_size

    @staticmethod
    def log_training_step(epoch, total_epochs, batch_idx, total_batches, 
                         stats, cost_time):
        """ç»Ÿä¸€çš„è®­ç»ƒæ—¥å¿—è¾“å‡º"""
        print(f"Epoch {epoch}/{total_epochs}, "
              f"Batch {batch_idx}/{total_batches}, "
              f"Tokens: {stats['avg_tokens']:.1f}, "
              f"Reward: {stats['avg_reward']:.3f}, "
              f"Loss: {stats['avg_loss']:.4f}, "
              f"Time: {cost_time:.2f}s")

    @staticmethod
    def save_checkpoint(model, tokenizer, epoch, output_dir):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = os.path.join(output_dir, f"checkpoint-epoch-{epoch}")
        os.makedirs(checkpoint_dir, exist_ok=True)
        model.save_pretrained(checkpoint_dir)
        tokenizer.save_pretrained(checkpoint_dir)
        return checkpoint_dir

class SimpleGRPOTrainer:
    """ç®€åŒ–çš„GRPOè®­ç»ƒå™¨ - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.device = config.get_device()
        self.metrics_tracker = MetricsTracker()
        self.model = None
        self.tokenizer = None
        self.optimizer = None
        
        # åˆå§‹åŒ–
        set_seed(config.seed)
        self._print_device_info()
        
    def _print_device_info(self):
        """æ‰“å°è®¾å¤‡ä¿¡æ¯"""
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        if self.config.use_mps:
            print("MPSå¯ç”¨ï¼Œå°†ä½¿ç”¨Apple Silicon GPUåŠ é€Ÿ")
        
    def setup(self):
        """ç»Ÿä¸€çš„è®¾ç½®æ–¹æ³•"""
        print("æ­£åœ¨è®¾ç½®æ¨¡å‹å’Œæ•°æ®...")
        
        # è®¾ç½®æ¨¡å‹
        self.model, self.tokenizer = setup_model_and_tokenizer(self.config)
        self.model.to(self.device)
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        self.optimizer = AdamW(
            self.model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )
        
        # è®¾ç½®æ•°æ®é›†
        self.dataset = load_math_dataset(self.config)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.config.output_dir, exist_ok=True)
        os.makedirs(self.config.logging_dir, exist_ok=True)
        
        print(f"è®¾ç½®å®Œæˆ - æ¨¡å‹å·²åŠ è½½ï¼Œæ•°æ®é›†: {len(self.dataset)}æ¡æ ·æœ¬")
        
    def generate_and_evaluate(self, question: str, ground_truth: str) -> Dict:
        """ç”Ÿæˆå“åº”å¹¶è¯„ä¼° - åˆå¹¶ç›¸å…³æ“ä½œ"""
        # å‡†å¤‡è¾“å…¥
        prompt = format_math_chat_input(question, self.tokenizer)
        inputs = self.tokenizer(prompt, return_tensors="pt", 
                               truncation=True, max_length=self.config.max_length).to(self.device)
        
        # ç”Ÿæˆå“åº”
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config.max_length,
                do_sample=True,
                temperature=0.7,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        # æå–å’Œè§£ç å“åº”
        input_length = inputs['input_ids'].shape[1]
        response_ids = outputs[0][input_length:]
        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)
        
        # è¯„ä¼°å“åº”
        predicted_answer = extract_predicted_answer(response)
        reward = compute_reward(predicted_answer, ground_truth)
        
        return {
            'prompt': prompt,
            'response': response,
            'token_count': len(response_ids),
            'input_ids': inputs['input_ids'],
            'response_ids': response_ids,
            'predicted_answer': predicted_answer,
            'ground_truth': ground_truth,
            'reward': reward
        }
    
    def compute_policy_loss(self, input_ids, response_ids, reward):
        """è®¡ç®—ç­–ç•¥æŸå¤± - ç®€åŒ–ç‰ˆæœ¬"""
        if len(response_ids) == 0:
            return None, 0.0
            
        # ç¡®ä¿å½¢çŠ¶æ­£ç¡®
        if response_ids.dim() == 1:
            response_ids = response_ids.unsqueeze(0)
        
        # å‰å‘ä¼ æ’­
        full_ids = torch.cat([input_ids, response_ids], dim=1)
        outputs = self.model(full_ids)
        
        # è®¡ç®—æŸå¤±
        prompt_length = input_ids.shape[1]
        response_length = response_ids.shape[1]
        
        # æå–å“åº”éƒ¨åˆ†çš„logitså¹¶è®¡ç®—æ¦‚ç‡
        response_logits = outputs.logits[:, prompt_length-1:prompt_length+response_length-1]
        log_probs = F.log_softmax(response_logits, dim=-1)
        selected_log_probs = log_probs[0].gather(1, response_ids[0].unsqueeze(1)).squeeze()
        
        # ç­–ç•¥æŸå¤±
        current_log_prob = selected_log_probs.mean()
        policy_loss = -current_log_prob * reward
        
        return policy_loss, current_log_prob.item()
    
    def train_step(self, batch_data) -> Dict:
        """æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤ - ä¼˜åŒ–ç‰ˆæœ¬"""
        total_loss = 0
        total_reward = 0
        total_tokens = 0
        valid_samples = 0

        for item in batch_data:
            # ç”Ÿæˆå’Œè¯„ä¼°
            result = self.generate_and_evaluate(
                item['question'], 
                extract_answer(item['answer'])
            )

            if result['reward'] < 0:
                print(f"prompt: {result['prompt']}")
                print(f"response: {result['response']}")

            print(f"predicted_answer: {result['predicted_answer']}")
            print(f"ground_truth: {result['ground_truth']}")
            print(f"reward: {result['reward']}")
            print(f"token_count: {result['token_count']}")
            
            # è®¡ç®—æŸå¤±å¹¶æ›´æ–°
            loss, log_prob = self.compute_policy_loss(
                result['input_ids'],
                result['response_ids'], 
                result['reward']
            )
            
            if loss is not None:
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)
                self.optimizer.step()
                
                total_loss += loss.item()
                valid_samples += 1
            
            total_reward += result['reward']
            total_tokens += result['token_count']
        
        # è¿”å›å¹³å‡ç»Ÿè®¡
        batch_size = len(batch_data)
        return {
            'avg_loss': total_loss / max(valid_samples, 1),
            'avg_reward': total_reward / batch_size,
            'avg_tokens': total_tokens / batch_size
        }
        
    def train_epoch(self, epoch: int) -> Dict:
        """è®­ç»ƒä¸€ä¸ªepoch - ç®€åŒ–ç‰ˆæœ¬"""
        print(f"\nå¼€å§‹è®­ç»ƒ Epoch {epoch + 1}/{self.config.num_train_epochs}")
        
        self.model.train()
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader, batch_size = TrainingUtils.create_dataloader(self.dataset, self.config)
        print(f"æ‰¹å¤„ç†å¤§å°: {batch_size}, æ‰¹æ¬¡æ•°: {len(dataloader)}")
        
        # è®­ç»ƒæŒ‡æ ‡
        epoch_metrics = {'token_counts': [], 'rewards': [], 'losses': []}
        
        for batch_idx, batch in enumerate(dataloader):
            # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
            start_time = time.time()
            step_stats = self.train_step(batch)
            cost_time = time.time() - start_time
            
            # è®°å½•æŒ‡æ ‡
            current_step = epoch * len(dataloader) + batch_idx
            self.metrics_tracker.add_metrics(
                current_step, step_stats['avg_tokens'], 
                step_stats['avg_reward'], step_stats['avg_loss']
            )
            
            for key in epoch_metrics:
                epoch_metrics[key].append(step_stats[f"avg_{key[:-1]}"])
            
            # æ—¥å¿—è¾“å‡º
            if (batch_idx + 1) % self.config.logging_steps == 0:
                TrainingUtils.log_training_step(
                    epoch + 1, self.config.num_train_epochs,
                    batch_idx + 1, len(dataloader),
                    step_stats, cost_time
                )
        
        return {f"avg_{k[:-1]}": np.mean(v) for k, v in epoch_metrics.items()}
        
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯ - ç®€åŒ–ç‰ˆæœ¬"""
        print("="*60)
        print("å¼€å§‹GRPOè®­ç»ƒ")
        print("="*60)
        
        # ç»Ÿä¸€è®¾ç½®
        self.setup()
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(self.config.num_train_epochs):
            epoch_stats = self.train_epoch(epoch)
            
            # è¾“å‡ºepochç»Ÿè®¡
            print(f"\nEpoch {epoch + 1} å®Œæˆ:")
            for key, value in epoch_stats.items():
                print(f"  {key}: {value:.3f}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            checkpoint_dir = TrainingUtils.save_checkpoint(
                self.model, self.tokenizer, epoch + 1, self.config.output_dir
            )
            print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_dir}")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self._save_final_results()
        print("è®­ç»ƒå®Œæˆï¼")
        
    def _save_final_results(self):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_dir = os.path.join(self.config.output_dir, "final_model")
        self.model.save_pretrained(final_model_dir)
        self.tokenizer.save_pretrained(final_model_dir)
        
        # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
        metrics_dict = self.metrics_tracker.save_to_dict()
        save_metrics(metrics_dict, self.config.output_dir)
        
        print(f"æœ€ç»ˆæ¨¡å‹ä¿å­˜è‡³: {final_model_dir}")

def main():
    """ä¸»å‡½æ•° - ç®€åŒ–ç‰ˆæœ¬"""
    print("ğŸš€ åˆå§‹åŒ–GRPOè®­ç»ƒ...")
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨å¹¶è®­ç»ƒ
        config = TrainingConfig()
        trainer = SimpleGRPOTrainer(config)
        trainer.train()
        
        # è®­ç»ƒæ€»ç»“
        print("\nğŸ“Š è®­ç»ƒæ€»ç»“:")
        final_stats = trainer.metrics_tracker.get_averages()
        for key, value in final_stats.items():
            print(f"  {key}: {value:.3f}")
        
        print("\nğŸ’¡ è¿è¡Œå¯è§†åŒ–: python visualize.py")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        raise

if __name__ == "__main__":
    main() 